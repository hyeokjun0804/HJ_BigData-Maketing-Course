{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iPZ_hWzGgdEZneFB6LCazrnz3jv-0jZ9",
      "authorship_tag": "ABX9TyOQD5Dhlel8sJIOVIxq0L7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeokjun0804/HJ_BigData_Maketing_Course/blob/main/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%A7%88%EC%BC%80%ED%8C%85_%EB%B6%84%EC%84%9D_%EC%A0%84%EB%AC%B8%EA%B0%80_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#빅데이터 마케팅 분석 전문가 4\n",
        "#2023년 1월 11일"
      ],
      "metadata": {
        "id": "9IPtoblTa-kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.인공신경망"
      ],
      "metadata": {
        "id": "-hcEFOFLkh7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y-nsrueQa5Hy"
      },
      "outputs": [],
      "source": [
        "#분류 에측\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Ashopping.csv\", encoding = \"cp949\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "H5OnsRPvbugf",
        "outputId": "277cd6ca-f243-4fae-a0ba-94097ed99fdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     고객ID  이탈여부      총매출액  구매금액대  방문빈도  1회 평균매출액  할인권 사용 횟수  총 할인 금액  고객등급  \\\n",
              "0       1     1   4963160      1    22    225598          1     5445     1   \n",
              "1       2     1   2271520      0    38     59777         22   350995     1   \n",
              "2       3     1   2484250      0     2   1242125          6   186045     1   \n",
              "3       4     1   2778850      0     9    308761          1     5195     2   \n",
              "4       5     1   4437610      1    10    443761          9   246350     2   \n",
              "..    ...   ...       ...    ...   ...       ...        ...      ...   ...   \n",
              "995   996     0  12731560      2    35    363759          3    44445     2   \n",
              "996   997     0   3419010      0     6    569835          6   186545     1   \n",
              "997   998     0   4612100      1    13    354777          8   260700     2   \n",
              "998   999     0   5165360      1    28    184477         20   347700     1   \n",
              "999  1000     0   4323050      1    16    270191         19   367045     2   \n",
              "\n",
              "     구매유형  ...  구매카테고리수  거주지역  성별  고객 나이대  거래기간  할인민감여부  Recency  Frequency  \\\n",
              "0       3  ...        7     6   1       6  3303       0        3          2   \n",
              "1       1  ...        5     4   0       2  3129       0        5          3   \n",
              "2       2  ...        4     6   0       3  3636       0        4          1   \n",
              "3       4  ...        4     5   0       4  3637       0        7          4   \n",
              "4       4  ...        6     6   0       4  3638       0        5          1   \n",
              "..    ...  ...      ...   ...  ..     ...   ...     ...      ...        ...   \n",
              "995     3  ...        8     6   0       4  4629       1        3          3   \n",
              "996     2  ...        9     6   0       4  4630       1        7          3   \n",
              "997     1  ...        1     4   1       6  4631       1        2          1   \n",
              "998     3  ...        7     5   1       6  3907       0        7          2   \n",
              "999     4  ...        6     5   1       6  2118       0        5          1   \n",
              "\n",
              "     Monetary      평균 구매주기  \n",
              "0           1   149.136364  \n",
              "1           4    81.342105  \n",
              "2           4  1817.000000  \n",
              "3           6   403.111111  \n",
              "4           4   362.800000  \n",
              "..        ...          ...  \n",
              "995         1   131.257143  \n",
              "996         6   770.666667  \n",
              "997         2   355.230769  \n",
              "998         6   138.535714  \n",
              "999         4   131.375000  \n",
              "\n",
              "[1000 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-156d6adc-5c31-443b-8bad-1d124fd5c999\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>고객ID</th>\n",
              "      <th>이탈여부</th>\n",
              "      <th>총매출액</th>\n",
              "      <th>구매금액대</th>\n",
              "      <th>방문빈도</th>\n",
              "      <th>1회 평균매출액</th>\n",
              "      <th>할인권 사용 횟수</th>\n",
              "      <th>총 할인 금액</th>\n",
              "      <th>고객등급</th>\n",
              "      <th>구매유형</th>\n",
              "      <th>...</th>\n",
              "      <th>구매카테고리수</th>\n",
              "      <th>거주지역</th>\n",
              "      <th>성별</th>\n",
              "      <th>고객 나이대</th>\n",
              "      <th>거래기간</th>\n",
              "      <th>할인민감여부</th>\n",
              "      <th>Recency</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Monetary</th>\n",
              "      <th>평균 구매주기</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4963160</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>225598</td>\n",
              "      <td>1</td>\n",
              "      <td>5445</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3303</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>149.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2271520</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>59777</td>\n",
              "      <td>22</td>\n",
              "      <td>350995</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3129</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>81.342105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2484250</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1242125</td>\n",
              "      <td>6</td>\n",
              "      <td>186045</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3636</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1817.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2778850</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>308761</td>\n",
              "      <td>1</td>\n",
              "      <td>5195</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3637</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>403.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4437610</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>443761</td>\n",
              "      <td>9</td>\n",
              "      <td>246350</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3638</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>362.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>996</td>\n",
              "      <td>0</td>\n",
              "      <td>12731560</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>363759</td>\n",
              "      <td>3</td>\n",
              "      <td>44445</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4629</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>131.257143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>997</td>\n",
              "      <td>0</td>\n",
              "      <td>3419010</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>569835</td>\n",
              "      <td>6</td>\n",
              "      <td>186545</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4630</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>770.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>998</td>\n",
              "      <td>0</td>\n",
              "      <td>4612100</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>354777</td>\n",
              "      <td>8</td>\n",
              "      <td>260700</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4631</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>355.230769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>5165360</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>184477</td>\n",
              "      <td>20</td>\n",
              "      <td>347700</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3907</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>138.535714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "      <td>4323050</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>270191</td>\n",
              "      <td>19</td>\n",
              "      <td>367045</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2118</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>131.375000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-156d6adc-5c31-443b-8bad-1d124fd5c999')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-156d6adc-5c31-443b-8bad-1d124fd5c999 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-156d6adc-5c31-443b-8bad-1d124fd5c999');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkM9WPlYbxJd",
        "outputId": "33c7d334-12cc-42a5-9d40-e4a28aa1e415"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   고객ID       1000 non-null   int64  \n",
            " 1   이탈여부       1000 non-null   int64  \n",
            " 2   총매출액       1000 non-null   int64  \n",
            " 3   구매금액대      1000 non-null   int64  \n",
            " 4   방문빈도       1000 non-null   int64  \n",
            " 5   1회 평균매출액   1000 non-null   int64  \n",
            " 6   할인권 사용 횟수  1000 non-null   int64  \n",
            " 7   총 할인 금액    1000 non-null   int64  \n",
            " 8   고객등급       1000 non-null   int64  \n",
            " 9   구매유형       1000 non-null   int64  \n",
            " 10  클레임접수여부    1000 non-null   int64  \n",
            " 11  구매카테고리수    1000 non-null   int64  \n",
            " 12  거주지역       1000 non-null   int64  \n",
            " 13  성별         1000 non-null   int64  \n",
            " 14  고객 나이대     1000 non-null   int64  \n",
            " 15  거래기간       1000 non-null   int64  \n",
            " 16  할인민감여부     1000 non-null   int64  \n",
            " 17  Recency    1000 non-null   int64  \n",
            " 18  Frequency  1000 non-null   int64  \n",
            " 19  Monetary   1000 non-null   int64  \n",
            " 20  평균 구매주기    1000 non-null   float64\n",
            "dtypes: float64(1), int64(20)\n",
            "memory usage: 164.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Foe2ZDyb7r-",
        "outputId": "45c64300-1cc5-4f30-ab6b-213853742b6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "고객ID         0\n",
              "이탈여부         0\n",
              "총매출액         0\n",
              "구매금액대        0\n",
              "방문빈도         0\n",
              "1회 평균매출액     0\n",
              "할인권 사용 횟수    0\n",
              "총 할인 금액      0\n",
              "고객등급         0\n",
              "구매유형         0\n",
              "클레임접수여부      0\n",
              "구매카테고리수      0\n",
              "거주지역         0\n",
              "성별           0\n",
              "고객 나이대       0\n",
              "거래기간         0\n",
              "할인민감여부       0\n",
              "Recency      0\n",
              "Frequency    0\n",
              "Monetary     0\n",
              "평균 구매주기      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "IVnuGSEkb-Ig",
        "outputId": "4303a7d8-b58e-4fcc-de10-810f897caf71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              고객ID         이탈여부          총매출액        구매금액대        방문빈도  \\\n",
              "count  1000.000000  1000.000000  1.000000e+03  1000.000000  1000.00000   \n",
              "mean    500.500000     0.300000  5.858013e+06     0.700000    22.91100   \n",
              "std     288.819436     0.458487  5.812815e+06     0.781416    19.08217   \n",
              "min       1.000000     0.000000  1.886100e+06     0.000000     1.00000   \n",
              "25%     250.750000     0.000000  2.815905e+06     0.000000    10.75000   \n",
              "50%     500.500000     0.000000  4.092145e+06     0.500000    18.00000   \n",
              "75%     750.250000     1.000000  6.545392e+06     1.000000    28.00000   \n",
              "max    1000.000000     1.000000  6.759576e+07     2.000000   155.00000   \n",
              "\n",
              "           1회 평균매출액    할인권 사용 횟수        총 할인 금액         고객등급         구매유형  \\\n",
              "count  1.000000e+03  1000.000000    1000.000000  1000.000000  1000.000000   \n",
              "mean   3.521024e+05    16.027000  292371.670000     1.546000     2.656000   \n",
              "std    3.124636e+05     8.341334  111937.501042     0.498129     1.046307   \n",
              "min    2.708200e+04     1.000000    3750.000000     1.000000     1.000000   \n",
              "25%    1.631242e+05     9.000000  261686.250000     1.000000     2.000000   \n",
              "50%    2.582080e+05    17.000000  347500.000000     2.000000     2.000000   \n",
              "75%    4.268310e+05    23.000000  365400.000000     2.000000     4.000000   \n",
              "max    2.798500e+06    30.000000  400600.000000     2.000000     4.000000   \n",
              "\n",
              "       ...      구매카테고리수         거주지역           성별       고객 나이대         거래기간  \\\n",
              "count  ...  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean   ...     5.217000     5.147000     0.189000     3.964000  3495.891000   \n",
              "std    ...     2.224153     1.169084     0.391705     1.078827   965.966194   \n",
              "min    ...     1.000000     1.000000     0.000000     2.000000   827.000000   \n",
              "25%    ...     3.000000     4.000000     0.000000     3.000000  2871.000000   \n",
              "50%    ...     5.000000     5.000000     0.000000     4.000000  3836.000000   \n",
              "75%    ...     7.000000     6.000000     0.000000     5.000000  4207.250000   \n",
              "max    ...     9.000000     7.000000     1.000000     7.000000  5334.000000   \n",
              "\n",
              "            할인민감여부      Recency    Frequency     Monetary      평균 구매주기  \n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
              "mean      0.400000     4.925000     2.289000     4.129000   266.880824  \n",
              "std       0.490143     1.744253     1.669811     1.560383   254.077398  \n",
              "min       0.000000     1.000000     1.000000     1.000000    13.980645  \n",
              "25%       0.000000     4.000000     1.000000     3.000000   111.957671  \n",
              "50%       0.000000     5.000000     2.000000     4.000000   191.469697  \n",
              "75%       1.000000     7.000000     3.000000     6.000000   324.386218  \n",
              "max       1.000000     7.000000     7.000000     7.000000  1956.000000  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15481a97-6055-4d4e-abf2-f86c1861f7f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>고객ID</th>\n",
              "      <th>이탈여부</th>\n",
              "      <th>총매출액</th>\n",
              "      <th>구매금액대</th>\n",
              "      <th>방문빈도</th>\n",
              "      <th>1회 평균매출액</th>\n",
              "      <th>할인권 사용 횟수</th>\n",
              "      <th>총 할인 금액</th>\n",
              "      <th>고객등급</th>\n",
              "      <th>구매유형</th>\n",
              "      <th>...</th>\n",
              "      <th>구매카테고리수</th>\n",
              "      <th>거주지역</th>\n",
              "      <th>성별</th>\n",
              "      <th>고객 나이대</th>\n",
              "      <th>거래기간</th>\n",
              "      <th>할인민감여부</th>\n",
              "      <th>Recency</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Monetary</th>\n",
              "      <th>평균 구매주기</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1.000000e+03</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>1.000000e+03</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>500.500000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>5.858013e+06</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>22.91100</td>\n",
              "      <td>3.521024e+05</td>\n",
              "      <td>16.027000</td>\n",
              "      <td>292371.670000</td>\n",
              "      <td>1.546000</td>\n",
              "      <td>2.656000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.217000</td>\n",
              "      <td>5.147000</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>3.964000</td>\n",
              "      <td>3495.891000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>4.925000</td>\n",
              "      <td>2.289000</td>\n",
              "      <td>4.129000</td>\n",
              "      <td>266.880824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>288.819436</td>\n",
              "      <td>0.458487</td>\n",
              "      <td>5.812815e+06</td>\n",
              "      <td>0.781416</td>\n",
              "      <td>19.08217</td>\n",
              "      <td>3.124636e+05</td>\n",
              "      <td>8.341334</td>\n",
              "      <td>111937.501042</td>\n",
              "      <td>0.498129</td>\n",
              "      <td>1.046307</td>\n",
              "      <td>...</td>\n",
              "      <td>2.224153</td>\n",
              "      <td>1.169084</td>\n",
              "      <td>0.391705</td>\n",
              "      <td>1.078827</td>\n",
              "      <td>965.966194</td>\n",
              "      <td>0.490143</td>\n",
              "      <td>1.744253</td>\n",
              "      <td>1.669811</td>\n",
              "      <td>1.560383</td>\n",
              "      <td>254.077398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.886100e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>2.708200e+04</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3750.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>827.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.980645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>250.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.815905e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.75000</td>\n",
              "      <td>1.631242e+05</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>261686.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2871.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>111.957671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>500.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.092145e+06</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>18.00000</td>\n",
              "      <td>2.582080e+05</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>347500.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3836.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>191.469697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>750.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.545392e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.00000</td>\n",
              "      <td>4.268310e+05</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>365400.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4207.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>324.386218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.759576e+07</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>155.00000</td>\n",
              "      <td>2.798500e+06</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>400600.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>5334.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1956.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15481a97-6055-4d4e-abf2-f86c1861f7f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15481a97-6055-4d4e-abf2-f86c1861f7f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15481a97-6055-4d4e-abf2-f86c1861f7f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "BxsHmshOcAiV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHM6jA1pczc7",
        "outputId": "54854811-8c1a-4fa2-a083-4a821df8b53c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['고객ID', '이탈여부', '총매출액', '구매금액대', '방문빈도', '1회 평균매출액', '할인권 사용 횟수',\n",
              "       '총 할인 금액', '고객등급', '구매유형', '클레임접수여부', '구매카테고리수', '거주지역', '성별', '고객 나이대',\n",
              "       '거래기간', '할인민감여부', 'Recency', 'Frequency', 'Monetary', '평균 구매주기'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#분류예측\n",
        "\n",
        "#1.변수선택\n",
        "X = df[['총매출액', '구매금액대','1회 평균매출액','평균 구매주기','거래기간']]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['총매출액','1회 평균매출액','평균 구매주기','거래기간']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['구매금액대'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4오버샘플링\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "qdV297lUcViZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 모델링\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nn_model = MLPClassifier(random_state= 0, alpha=0.001, hidden_layer_sizes = [50])\n",
        "#5-1. 모형학습\n",
        "nn_model.fit(X_train, Y_train)\n",
        "\n",
        "#6 예측\n",
        "\n",
        "Y_pred = nn_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w1z6_tUeOGx",
        "outputId": "c0a62f3a-8fd4-4a78-841a-38e12046c6c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 결과값 보고\n",
        "print(\"Y 예측값\\n\", Y_pred)\n",
        "print(\"accuracy(train): {:.3f}\".format(nn_model.score(X_train, Y_train)))\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tfuVSrxhKlA",
        "outputId": "a2e3dd25-00e5-448f-a085-e075524d6c0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y 예측값\n",
            " [1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(train): 0.889\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.91       177\n",
            "           1       0.85      0.89      0.87       123\n",
            "\n",
            "    accuracy                           0.89       300\n",
            "   macro avg       0.89      0.89      0.89       300\n",
            "weighted avg       0.89      0.89      0.89       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#1.변수선택\n",
        "X = df[df.이탈여부 ==0][['방문빈도', '총 할인 금액','고객등급','구매유형','거래기간','할인민감여부','평균 구매주기']]\n",
        "Y = df[df.이탈여부 ==0][\"1회 평균매출액\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['방문빈도', '총 할인 금액','거래기간','평균 구매주기']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['고객등급','구매유형','할인민감여부'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)"
      ],
      "metadata": {
        "id": "0M8pyN5Ch5Up"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 모델링\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "#4-1.모델 생성\n",
        "nn_reg_model = MLPRegressor(random_state= 0, alpha=1, max_iter= 1000,\n",
        "                            hidden_layer_sizes=[50, 50])\n",
        "\n",
        "#5.모형학습 및 예측\n",
        "nn_reg_model.fit(X_train, Y_train)\n",
        "Y_pred = nn_reg_model.predict(X_test)\n",
        "\n",
        "#6.결과값 보고\n",
        "print(\"Y predict value: \\n\", Y_pred)\n",
        "print(\"train accuracyL {:.3f}\".format(nn_reg_model.score(X_train, Y_train)))\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE:{:.3f}\".format(rmse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAn7euWip2Is",
        "outputId": "5391137d-26ba-4aca-fbc3-d272267cd49f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value: \n",
            " [367567.22799569 291986.7171292  286542.39507057 470299.18982825\n",
            " 199015.43022523 314985.16674704 120385.261673    99679.50744792\n",
            " 217207.85032309 321283.5694424  322281.24636508 139031.08090941\n",
            " 172479.30691299 307474.28058998 410674.7673535  282802.14200659\n",
            " 314022.43590656 344082.61534343 266080.18516834 299527.46258462\n",
            " 417918.96528793 213628.82138014 325703.16436337 276841.23973322\n",
            "  83289.89193415 370306.40166324 392255.59988171 148021.81268408\n",
            " 135257.54780785 288250.40948644 329672.79002503 409898.41100752\n",
            " 238134.15585775 368306.05176262 275290.22641391 239081.40629288\n",
            " 266321.69500888 375448.00373975 246605.1330584  641448.06611111\n",
            " 336904.01756403 215252.99856889 241609.33155848 271063.86770618\n",
            " 236907.37454933 382141.50786421 141280.68282403 289125.50608539\n",
            " 298377.15265302 346302.09403242 253397.19897089 508363.2921946\n",
            "  55430.52886374 316398.6845584  307992.35916116 291458.05176895\n",
            " 365830.87156821 337348.7566333  283210.3790915  258622.02741059\n",
            " 321376.87461064 327403.5219942  223207.07109362 339375.24930473\n",
            " 235633.20353306 207937.38459674 262238.13165728 155176.57248646\n",
            " 446908.94216759 158813.06040685 324047.59496836 252640.34325201\n",
            " 249188.03432076 255476.98860462 135204.18190746 486863.31488025\n",
            " 389836.25584169 417264.89960339 137349.62110706 330482.67927447\n",
            " 202882.37492681 244085.71728165 216194.80179372 178187.54506876\n",
            " 326307.74557151 334835.24310067 259056.39725013 318455.2747672\n",
            " 245978.7402989  349977.88045537 313576.00711254 360034.97890785\n",
            " 302180.47800664 346414.147532   237623.24093074 537825.48286654\n",
            " 306899.04560765 198016.87086714 356756.250312   126411.60509786\n",
            " 222199.09532434 293326.26886908 357988.60770913 274905.50147079\n",
            " 321045.37222521 308870.73491254 328625.82189502 200975.70277517\n",
            " 296192.52973971 279928.30765362 260548.00636601 243793.66083518\n",
            " 342931.62784114 486251.93814507 263093.75393189 222741.17780968\n",
            " 280110.46803518 314221.44235756 237481.55329132 271619.53638204\n",
            " 356806.58975663 367708.28247153 232120.08009347 279820.78777821\n",
            " 336140.0816819  301035.34267013 196964.58034718 242703.54496087\n",
            " 382656.27941644 264746.6531743   74865.40367927 229185.42533768\n",
            " 449451.21775471 333497.22792232 181490.12700764 308568.97923415\n",
            " 162126.67867317 273979.725792   312484.58403589 268972.60081153\n",
            " 294422.26870008 280549.89684759 369158.98151701 236824.03178162\n",
            " 191405.47832004 351443.85315144 298057.17180239 366323.17194185\n",
            " 160087.98542242 289201.47993591 187648.53842493 275254.96461994\n",
            " 296878.85455016 351165.94366769 685012.47921711 197831.35625512\n",
            "    817.42017899 336795.26572669 307319.33036176 286538.74627755\n",
            " 356601.98981613 413427.13794709 399347.35085261 181438.83136968\n",
            " 229242.10355842 330888.72112284 326911.57257259 213774.88283495\n",
            " 372421.41967877 243911.48617397   1134.30582295 251342.72805545\n",
            " 149121.8720192  518450.65557447 163722.93239689 330273.5210797\n",
            " 628305.94584286 229238.61156581 308547.9272714  150580.26882881\n",
            " 557029.72398692 782868.63100081 389620.35786206 424001.85956249\n",
            " 261503.41228481 201832.48696299 311830.67263387 308643.85612775\n",
            "  63366.06569213 343396.35371872 309710.08763841 315943.40002105\n",
            " 590729.84011205  81729.53183386 282024.21237733 289056.00731604\n",
            " 329795.69550524 107196.24435223 373365.59284941 119783.54874409\n",
            " 474729.87515126 108723.20464901 294099.46394641 263585.28216134\n",
            " 282962.56218265 223533.52354009 317057.78433315 328600.46139415\n",
            " 342669.79765667 293334.82175315]\n",
            "train accuracyL 0.300\n",
            "RMSE:227578.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#딥러닝"
      ],
      "metadata": {
        "id": "UISDmhKwuIL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###경고문 제거"
      ],
      "metadata": {
        "id": "eUmcwTRYuLpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #딥러닝을 제거하겠다."
      ],
      "metadata": {
        "id": "7vE4V1UhrLYf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN(Deep learning: Deep Neural Network"
      ],
      "metadata": {
        "id": "qeN1VhBQuX_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#분류예측\n",
        "\n",
        "#1.변수선택\n",
        "X = df[['총매출액', '구매금액대','1회 평균매출액','평균 구매주기','거래기간']]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3.데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['총매출액','1회 평균매출액','평균 구매주기','거래기간']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['구매금액대'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4.오버샘플링\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "o4LgZLbAuXLv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.metrics import Accuracy\n",
        "\n",
        "#5.시드값 설정: 시작점\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "#6 모형생성\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(64, input_dim=7, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "#7.모형 학습\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, Y_train, validation_split = 0.2, epochs = 100,\n",
        "                    batch_size = 64, verbose = 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grSFP0Eouoam",
        "outputId": "6b89badc-432a-486a-d180-a3b4284981d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11/11 - 1s - loss: 0.6829 - accuracy: 0.5888 - val_loss: 0.6856 - val_accuracy: 0.6529 - 1s/epoch - 112ms/step\n",
            "Epoch 2/100\n",
            "11/11 - 0s - loss: 0.5836 - accuracy: 0.8254 - val_loss: 0.6330 - val_accuracy: 0.6471 - 75ms/epoch - 7ms/step\n",
            "Epoch 3/100\n",
            "11/11 - 0s - loss: 0.5174 - accuracy: 0.8284 - val_loss: 0.6003 - val_accuracy: 0.6765 - 65ms/epoch - 6ms/step\n",
            "Epoch 4/100\n",
            "11/11 - 0s - loss: 0.4679 - accuracy: 0.8328 - val_loss: 0.5588 - val_accuracy: 0.7353 - 69ms/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "11/11 - 0s - loss: 0.4343 - accuracy: 0.8595 - val_loss: 0.5102 - val_accuracy: 0.8118 - 64ms/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "11/11 - 0s - loss: 0.4104 - accuracy: 0.8624 - val_loss: 0.5205 - val_accuracy: 0.7882 - 78ms/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "11/11 - 0s - loss: 0.3902 - accuracy: 0.8713 - val_loss: 0.4881 - val_accuracy: 0.8235 - 63ms/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "11/11 - 0s - loss: 0.3756 - accuracy: 0.8802 - val_loss: 0.4970 - val_accuracy: 0.8176 - 69ms/epoch - 6ms/step\n",
            "Epoch 9/100\n",
            "11/11 - 0s - loss: 0.3606 - accuracy: 0.8905 - val_loss: 0.4988 - val_accuracy: 0.8235 - 69ms/epoch - 6ms/step\n",
            "Epoch 10/100\n",
            "11/11 - 0s - loss: 0.3481 - accuracy: 0.8876 - val_loss: 0.4776 - val_accuracy: 0.8412 - 61ms/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "11/11 - 0s - loss: 0.3354 - accuracy: 0.8964 - val_loss: 0.5106 - val_accuracy: 0.8235 - 67ms/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "11/11 - 0s - loss: 0.3230 - accuracy: 0.8994 - val_loss: 0.4591 - val_accuracy: 0.8529 - 65ms/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "11/11 - 0s - loss: 0.3110 - accuracy: 0.9038 - val_loss: 0.4816 - val_accuracy: 0.8471 - 62ms/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "11/11 - 0s - loss: 0.3008 - accuracy: 0.9038 - val_loss: 0.4843 - val_accuracy: 0.8471 - 100ms/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "11/11 - 0s - loss: 0.2923 - accuracy: 0.9112 - val_loss: 0.4878 - val_accuracy: 0.8529 - 77ms/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "11/11 - 0s - loss: 0.2855 - accuracy: 0.9172 - val_loss: 0.4796 - val_accuracy: 0.8529 - 67ms/epoch - 6ms/step\n",
            "Epoch 17/100\n",
            "11/11 - 0s - loss: 0.2792 - accuracy: 0.9172 - val_loss: 0.4837 - val_accuracy: 0.8588 - 61ms/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "11/11 - 0s - loss: 0.2729 - accuracy: 0.9216 - val_loss: 0.4902 - val_accuracy: 0.8588 - 68ms/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "11/11 - 0s - loss: 0.2664 - accuracy: 0.9157 - val_loss: 0.4760 - val_accuracy: 0.8588 - 73ms/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "11/11 - 0s - loss: 0.2629 - accuracy: 0.9186 - val_loss: 0.4975 - val_accuracy: 0.8588 - 60ms/epoch - 5ms/step\n",
            "Epoch 21/100\n",
            "11/11 - 0s - loss: 0.2586 - accuracy: 0.9216 - val_loss: 0.4753 - val_accuracy: 0.8529 - 76ms/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "11/11 - 0s - loss: 0.2545 - accuracy: 0.9216 - val_loss: 0.4969 - val_accuracy: 0.8529 - 83ms/epoch - 8ms/step\n",
            "Epoch 23/100\n",
            "11/11 - 0s - loss: 0.2500 - accuracy: 0.9216 - val_loss: 0.4686 - val_accuracy: 0.8588 - 62ms/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "11/11 - 0s - loss: 0.2481 - accuracy: 0.9186 - val_loss: 0.4818 - val_accuracy: 0.8588 - 86ms/epoch - 8ms/step\n",
            "Epoch 25/100\n",
            "11/11 - 0s - loss: 0.2481 - accuracy: 0.9186 - val_loss: 0.4760 - val_accuracy: 0.8588 - 75ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "11/11 - 0s - loss: 0.2421 - accuracy: 0.9201 - val_loss: 0.4532 - val_accuracy: 0.8588 - 62ms/epoch - 6ms/step\n",
            "Epoch 27/100\n",
            "11/11 - 0s - loss: 0.2418 - accuracy: 0.9246 - val_loss: 0.4800 - val_accuracy: 0.8588 - 76ms/epoch - 7ms/step\n",
            "Epoch 28/100\n",
            "11/11 - 0s - loss: 0.2370 - accuracy: 0.9260 - val_loss: 0.4627 - val_accuracy: 0.8588 - 79ms/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "11/11 - 0s - loss: 0.2360 - accuracy: 0.9172 - val_loss: 0.4610 - val_accuracy: 0.8588 - 75ms/epoch - 7ms/step\n",
            "Epoch 30/100\n",
            "11/11 - 0s - loss: 0.2324 - accuracy: 0.9275 - val_loss: 0.4712 - val_accuracy: 0.8529 - 65ms/epoch - 6ms/step\n",
            "Epoch 31/100\n",
            "11/11 - 0s - loss: 0.2309 - accuracy: 0.9290 - val_loss: 0.4492 - val_accuracy: 0.8588 - 89ms/epoch - 8ms/step\n",
            "Epoch 32/100\n",
            "11/11 - 0s - loss: 0.2293 - accuracy: 0.9290 - val_loss: 0.4589 - val_accuracy: 0.8588 - 65ms/epoch - 6ms/step\n",
            "Epoch 33/100\n",
            "11/11 - 0s - loss: 0.2291 - accuracy: 0.9305 - val_loss: 0.4483 - val_accuracy: 0.8588 - 77ms/epoch - 7ms/step\n",
            "Epoch 34/100\n",
            "11/11 - 0s - loss: 0.2252 - accuracy: 0.9290 - val_loss: 0.4475 - val_accuracy: 0.8588 - 70ms/epoch - 6ms/step\n",
            "Epoch 35/100\n",
            "11/11 - 0s - loss: 0.2247 - accuracy: 0.9290 - val_loss: 0.4419 - val_accuracy: 0.8588 - 82ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "11/11 - 0s - loss: 0.2240 - accuracy: 0.9246 - val_loss: 0.4333 - val_accuracy: 0.8647 - 67ms/epoch - 6ms/step\n",
            "Epoch 37/100\n",
            "11/11 - 0s - loss: 0.2223 - accuracy: 0.9305 - val_loss: 0.4677 - val_accuracy: 0.8529 - 81ms/epoch - 7ms/step\n",
            "Epoch 38/100\n",
            "11/11 - 0s - loss: 0.2193 - accuracy: 0.9290 - val_loss: 0.4230 - val_accuracy: 0.8588 - 68ms/epoch - 6ms/step\n",
            "Epoch 39/100\n",
            "11/11 - 0s - loss: 0.2183 - accuracy: 0.9305 - val_loss: 0.4455 - val_accuracy: 0.8588 - 68ms/epoch - 6ms/step\n",
            "Epoch 40/100\n",
            "11/11 - 0s - loss: 0.2165 - accuracy: 0.9334 - val_loss: 0.4343 - val_accuracy: 0.8647 - 76ms/epoch - 7ms/step\n",
            "Epoch 41/100\n",
            "11/11 - 0s - loss: 0.2149 - accuracy: 0.9334 - val_loss: 0.4333 - val_accuracy: 0.8588 - 77ms/epoch - 7ms/step\n",
            "Epoch 42/100\n",
            "11/11 - 0s - loss: 0.2139 - accuracy: 0.9320 - val_loss: 0.4404 - val_accuracy: 0.8588 - 68ms/epoch - 6ms/step\n",
            "Epoch 43/100\n",
            "11/11 - 0s - loss: 0.2145 - accuracy: 0.9260 - val_loss: 0.4200 - val_accuracy: 0.8588 - 75ms/epoch - 7ms/step\n",
            "Epoch 44/100\n",
            "11/11 - 0s - loss: 0.2120 - accuracy: 0.9349 - val_loss: 0.4548 - val_accuracy: 0.8529 - 62ms/epoch - 6ms/step\n",
            "Epoch 45/100\n",
            "11/11 - 0s - loss: 0.2110 - accuracy: 0.9334 - val_loss: 0.4268 - val_accuracy: 0.8647 - 72ms/epoch - 7ms/step\n",
            "Epoch 46/100\n",
            "11/11 - 0s - loss: 0.2082 - accuracy: 0.9364 - val_loss: 0.4381 - val_accuracy: 0.8529 - 66ms/epoch - 6ms/step\n",
            "Epoch 47/100\n",
            "11/11 - 0s - loss: 0.2085 - accuracy: 0.9364 - val_loss: 0.4311 - val_accuracy: 0.8588 - 84ms/epoch - 8ms/step\n",
            "Epoch 48/100\n",
            "11/11 - 0s - loss: 0.2071 - accuracy: 0.9349 - val_loss: 0.4126 - val_accuracy: 0.8647 - 66ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "11/11 - 0s - loss: 0.2064 - accuracy: 0.9379 - val_loss: 0.4292 - val_accuracy: 0.8588 - 62ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "11/11 - 0s - loss: 0.2051 - accuracy: 0.9334 - val_loss: 0.4113 - val_accuracy: 0.8647 - 57ms/epoch - 5ms/step\n",
            "Epoch 51/100\n",
            "11/11 - 0s - loss: 0.2056 - accuracy: 0.9379 - val_loss: 0.4268 - val_accuracy: 0.8588 - 62ms/epoch - 6ms/step\n",
            "Epoch 52/100\n",
            "11/11 - 0s - loss: 0.2044 - accuracy: 0.9349 - val_loss: 0.4156 - val_accuracy: 0.8647 - 85ms/epoch - 8ms/step\n",
            "Epoch 53/100\n",
            "11/11 - 0s - loss: 0.2026 - accuracy: 0.9364 - val_loss: 0.4326 - val_accuracy: 0.8529 - 81ms/epoch - 7ms/step\n",
            "Epoch 54/100\n",
            "11/11 - 0s - loss: 0.2007 - accuracy: 0.9408 - val_loss: 0.4113 - val_accuracy: 0.8647 - 65ms/epoch - 6ms/step\n",
            "Epoch 55/100\n",
            "11/11 - 0s - loss: 0.1997 - accuracy: 0.9393 - val_loss: 0.4058 - val_accuracy: 0.8647 - 66ms/epoch - 6ms/step\n",
            "Epoch 56/100\n",
            "11/11 - 0s - loss: 0.1999 - accuracy: 0.9393 - val_loss: 0.4358 - val_accuracy: 0.8588 - 74ms/epoch - 7ms/step\n",
            "Epoch 57/100\n",
            "11/11 - 0s - loss: 0.1996 - accuracy: 0.9393 - val_loss: 0.3937 - val_accuracy: 0.8647 - 58ms/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "11/11 - 0s - loss: 0.1997 - accuracy: 0.9364 - val_loss: 0.4277 - val_accuracy: 0.8529 - 45ms/epoch - 4ms/step\n",
            "Epoch 59/100\n",
            "11/11 - 0s - loss: 0.1975 - accuracy: 0.9408 - val_loss: 0.3932 - val_accuracy: 0.8647 - 68ms/epoch - 6ms/step\n",
            "Epoch 60/100\n",
            "11/11 - 0s - loss: 0.1948 - accuracy: 0.9423 - val_loss: 0.4205 - val_accuracy: 0.8588 - 61ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "11/11 - 0s - loss: 0.1951 - accuracy: 0.9438 - val_loss: 0.4008 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 62/100\n",
            "11/11 - 0s - loss: 0.1935 - accuracy: 0.9408 - val_loss: 0.4069 - val_accuracy: 0.8647 - 57ms/epoch - 5ms/step\n",
            "Epoch 63/100\n",
            "11/11 - 0s - loss: 0.1947 - accuracy: 0.9423 - val_loss: 0.4142 - val_accuracy: 0.8588 - 70ms/epoch - 6ms/step\n",
            "Epoch 64/100\n",
            "11/11 - 0s - loss: 0.1929 - accuracy: 0.9438 - val_loss: 0.4039 - val_accuracy: 0.8647 - 67ms/epoch - 6ms/step\n",
            "Epoch 65/100\n",
            "11/11 - 0s - loss: 0.1912 - accuracy: 0.9453 - val_loss: 0.4085 - val_accuracy: 0.8647 - 47ms/epoch - 4ms/step\n",
            "Epoch 66/100\n",
            "11/11 - 0s - loss: 0.1911 - accuracy: 0.9438 - val_loss: 0.4082 - val_accuracy: 0.8647 - 66ms/epoch - 6ms/step\n",
            "Epoch 67/100\n",
            "11/11 - 0s - loss: 0.1901 - accuracy: 0.9438 - val_loss: 0.4114 - val_accuracy: 0.8588 - 67ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "11/11 - 0s - loss: 0.1910 - accuracy: 0.9408 - val_loss: 0.3896 - val_accuracy: 0.8647 - 56ms/epoch - 5ms/step\n",
            "Epoch 69/100\n",
            "11/11 - 0s - loss: 0.1892 - accuracy: 0.9453 - val_loss: 0.3962 - val_accuracy: 0.8647 - 65ms/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "11/11 - 0s - loss: 0.1887 - accuracy: 0.9408 - val_loss: 0.4095 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 71/100\n",
            "11/11 - 0s - loss: 0.1872 - accuracy: 0.9438 - val_loss: 0.3883 - val_accuracy: 0.8588 - 65ms/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "11/11 - 0s - loss: 0.1875 - accuracy: 0.9408 - val_loss: 0.3898 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 73/100\n",
            "11/11 - 0s - loss: 0.1867 - accuracy: 0.9482 - val_loss: 0.4074 - val_accuracy: 0.8588 - 67ms/epoch - 6ms/step\n",
            "Epoch 74/100\n",
            "11/11 - 0s - loss: 0.1880 - accuracy: 0.9393 - val_loss: 0.3759 - val_accuracy: 0.8647 - 84ms/epoch - 8ms/step\n",
            "Epoch 75/100\n",
            "11/11 - 0s - loss: 0.1884 - accuracy: 0.9438 - val_loss: 0.4138 - val_accuracy: 0.8588 - 84ms/epoch - 8ms/step\n",
            "Epoch 76/100\n",
            "11/11 - 0s - loss: 0.1838 - accuracy: 0.9423 - val_loss: 0.3702 - val_accuracy: 0.8647 - 81ms/epoch - 7ms/step\n",
            "Epoch 77/100\n",
            "11/11 - 0s - loss: 0.1842 - accuracy: 0.9467 - val_loss: 0.3963 - val_accuracy: 0.8588 - 76ms/epoch - 7ms/step\n",
            "Epoch 78/100\n",
            "11/11 - 0s - loss: 0.1831 - accuracy: 0.9497 - val_loss: 0.3934 - val_accuracy: 0.8588 - 58ms/epoch - 5ms/step\n",
            "Epoch 79/100\n",
            "11/11 - 0s - loss: 0.1830 - accuracy: 0.9467 - val_loss: 0.3960 - val_accuracy: 0.8588 - 70ms/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "11/11 - 0s - loss: 0.1824 - accuracy: 0.9423 - val_loss: 0.3860 - val_accuracy: 0.8647 - 61ms/epoch - 6ms/step\n",
            "Epoch 81/100\n",
            "11/11 - 0s - loss: 0.1808 - accuracy: 0.9482 - val_loss: 0.3950 - val_accuracy: 0.8588 - 71ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "11/11 - 0s - loss: 0.1814 - accuracy: 0.9482 - val_loss: 0.3916 - val_accuracy: 0.8588 - 71ms/epoch - 6ms/step\n",
            "Epoch 83/100\n",
            "11/11 - 0s - loss: 0.1797 - accuracy: 0.9497 - val_loss: 0.3803 - val_accuracy: 0.8647 - 77ms/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "11/11 - 0s - loss: 0.1793 - accuracy: 0.9467 - val_loss: 0.3954 - val_accuracy: 0.8588 - 80ms/epoch - 7ms/step\n",
            "Epoch 85/100\n",
            "11/11 - 0s - loss: 0.1794 - accuracy: 0.9453 - val_loss: 0.3924 - val_accuracy: 0.8588 - 63ms/epoch - 6ms/step\n",
            "Epoch 86/100\n",
            "11/11 - 0s - loss: 0.1789 - accuracy: 0.9482 - val_loss: 0.3679 - val_accuracy: 0.8588 - 57ms/epoch - 5ms/step\n",
            "Epoch 87/100\n",
            "11/11 - 0s - loss: 0.1777 - accuracy: 0.9497 - val_loss: 0.3876 - val_accuracy: 0.8588 - 50ms/epoch - 5ms/step\n",
            "Epoch 88/100\n",
            "11/11 - 0s - loss: 0.1773 - accuracy: 0.9512 - val_loss: 0.3835 - val_accuracy: 0.8588 - 59ms/epoch - 5ms/step\n",
            "Epoch 89/100\n",
            "11/11 - 0s - loss: 0.1784 - accuracy: 0.9438 - val_loss: 0.3948 - val_accuracy: 0.8588 - 50ms/epoch - 5ms/step\n",
            "Epoch 90/100\n",
            "11/11 - 0s - loss: 0.1765 - accuracy: 0.9527 - val_loss: 0.3870 - val_accuracy: 0.8588 - 48ms/epoch - 4ms/step\n",
            "Epoch 91/100\n",
            "11/11 - 0s - loss: 0.1758 - accuracy: 0.9497 - val_loss: 0.3666 - val_accuracy: 0.8647 - 46ms/epoch - 4ms/step\n",
            "Epoch 92/100\n",
            "11/11 - 0s - loss: 0.1772 - accuracy: 0.9423 - val_loss: 0.3699 - val_accuracy: 0.8588 - 70ms/epoch - 6ms/step\n",
            "Epoch 93/100\n",
            "11/11 - 0s - loss: 0.1755 - accuracy: 0.9512 - val_loss: 0.3941 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 94/100\n",
            "11/11 - 0s - loss: 0.1753 - accuracy: 0.9467 - val_loss: 0.3659 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 95/100\n",
            "11/11 - 0s - loss: 0.1743 - accuracy: 0.9512 - val_loss: 0.3836 - val_accuracy: 0.8588 - 47ms/epoch - 4ms/step\n",
            "Epoch 96/100\n",
            "11/11 - 0s - loss: 0.1743 - accuracy: 0.9482 - val_loss: 0.3720 - val_accuracy: 0.8588 - 53ms/epoch - 5ms/step\n",
            "Epoch 97/100\n",
            "11/11 - 0s - loss: 0.1739 - accuracy: 0.9527 - val_loss: 0.3804 - val_accuracy: 0.8588 - 90ms/epoch - 8ms/step\n",
            "Epoch 98/100\n",
            "11/11 - 0s - loss: 0.1749 - accuracy: 0.9423 - val_loss: 0.3558 - val_accuracy: 0.8647 - 199ms/epoch - 18ms/step\n",
            "Epoch 99/100\n",
            "11/11 - 0s - loss: 0.1722 - accuracy: 0.9482 - val_loss: 0.3841 - val_accuracy: 0.8588 - 210ms/epoch - 19ms/step\n",
            "Epoch 100/100\n",
            "11/11 - 0s - loss: 0.1716 - accuracy: 0.9497 - val_loss: 0.3719 - val_accuracy: 0.8588 - 157ms/epoch - 14ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "#loss(오차) 그리기\n",
        "loss_ax.plot(history.history[\"loss\"], \"y\", label = \"train loss\")\n",
        "loss_ax.plot(history.history[\"val_loss\"], \"r\", label=\"val loss\")\n",
        "loss_ax.set_xlabel(\"epoch\")\n",
        "loss_ax.set_ylabel(\"loss\")\n",
        "loss_ax.legend(loc = \"upper right\")\n",
        "\n",
        "#accuracy(정확도)  그리기\n",
        "acc_ax.plot(history.history[\"accuracy\"], \"b\", label = \"train acc\")\n",
        "acc_ax.plot(history.history[\"val_accuracy\"], \"g\", label = \"val acc\")\n",
        "acc_ax.set_ylabel(\"accuracy\")\n",
        "acc_ax.legend(loc = \"upper right\")\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "68umkJfbvHtn",
        "outputId": "af2c9bb8-5242-4c81-a5de-c391048ccc59"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test,verbose = 0)\n",
        "Y_class = np.round(Y_pred, 0)\n",
        "train_score = model.evaluate(X_train, Y_train, verbose=0)\n",
        "test_score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Y 예측값 확률: \\n\", Y_pred[:5])\n",
        "print(\"Y 예측 클래스: \\n\", Y_class[:5])\n",
        "print(\"train accuracy : {:.3f}\".format(train_score[0], train_score[1]))\n",
        "print(\"test accuracy : {:.3f}\".format(test_score[0], test_score[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3GcOVSf6KwL",
        "outputId": "0ed03a18-03d2-44c4-c246-73218fb4f81f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y 예측값 확률: \n",
            " [[0.929883  ]\n",
            " [1.        ]\n",
            " [0.03625259]\n",
            " [0.02216326]\n",
            " [0.99921256]]\n",
            "Y 예측 클래스: \n",
            " [[1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n",
            "train accuracy : 0.210\n",
            "test accuracy : 0.225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2023년 1월 12일"
      ],
      "metadata": {
        "id": "eY2Q7iqYT5Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 데이터 만들기\n",
        "X = df[df.이탈여부 ==0][['방문빈도','총 할인 금액','구매카테고리수','거래기간']]\n",
        "Y = np.log1p(df[df.이탈여부 ==0][\"1회 평균매출액\"])"
      ],
      "metadata": {
        "id": "JhVpz1MD8W9P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. train - test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=0)\n",
        "\n",
        "#3 데이터 전처러(preprocessing)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#4. seed 값 설정\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "#5. 모형생성\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(64, input_dim=4, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "#6. 모형 학습\n",
        "model.compile(loss = \"mse\", optimizer = \"SGD\")\n",
        "Y_pred = np.round(model.predict(X_test[:5], verbose=0), 3)\n",
        "print(\"Y predict value \\n\", Y_pred)\n",
        "\n",
        "#7. 모형 평가\n",
        "train_score = model.evaluate(X_train, Y_train,verbose=0)\n",
        "test_score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"train mse: {:.3f}\".format(train_score))\n",
        "print(\"test accuracy: {:.3f}\".format(test_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkboinCDV7bs",
        "outputId": "c9304e76-0c5f-44ec-e6bf-842d76cc91df"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [[0.553]\n",
            " [0.523]\n",
            " [0.607]\n",
            " [0.557]\n",
            " [0.476]]\n",
            "train mse: 141.936\n",
            "test accuracy: 141.488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.변수선택\n",
        "X = df[['총매출액', '구매금액대','1회 평균매출액','평균 구매주기','거래기간']]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3.데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['총매출액','1회 평균매출액','평균 구매주기','거래기간']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['구매금액대'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4.오버샘플링\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
        "\n",
        "#5. 단일모형 생성\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "dtree = DecisionTreeClassifier(random_state=0)\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "#6. 앙상블 모형 생성\n",
        "model = VotingClassifier(estimators=[(\"K-NN\",knn),(\"Dtree\",dtree)], voting = \"soft\")\n",
        "\n",
        "#7. 모형학습\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "#8. 예측 및 모형 성능 평가\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value: \\n\", Y_pred)\n",
        "#8-1. 보팅모형 정확도\n",
        "print(\"voting classifier accuracy: {0:.3f}\".format(model.score(X_test, Y_test)))\n",
        "#8-2. 개발모형 정확도\n",
        "classifiers = [dtree,knn]\n",
        "for classifier in classifiers :\n",
        "    classifier.fit(X_train,Y_train)\n",
        "    class_name = classifier.__class__.__name__\n",
        "    print(\"{0} accuracy : {1:.3f}\".format(class_name, classifier.score(X_test,Y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDRjqLkwYHwg",
        "outputId": "51627232-187a-4366-a809-ee6b74e2bdab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value: \n",
            " [1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 0]\n",
            "voting classifier accuracy: 0.907\n",
            "DecisionTreeClassifier accuracy : 0.900\n",
            "KNeighborsClassifier accuracy : 0.850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#회귀예측"
      ],
      "metadata": {
        "id": "gFBGs1O3oDqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#1.변수선택\n",
        "X = df[df.이탈여부 ==0][['방문빈도', '총 할인 금액','고객등급','구매유형','거래기간','할인민감여부','평균 구매주기']]\n",
        "Y = df[df.이탈여부 ==0][\"1회 평균매출액\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['방문빈도', '총 할인 금액','거래기간','평균 구매주기']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['고객등급','구매유형','할인민감여부'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4 단일 모형 생성\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svr = SVR()\n",
        "mlp = MLPRegressor(random_state = 0)\n",
        "\n",
        "#5. voting model 생성\n",
        "model = VotingRegressor(estimators=[(\"SVR\",svr),(\"MLP\",mlp)])\n",
        "\n",
        "#6. 모형 학습\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "#7.예측 및 평가\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value: \\n\", Y_pred)\n",
        "#8-1. 보팅모형 정확도\n",
        "print(\"voting regressor accuracy(R2): {0:.3f}\".format(model.score(X_test, Y_test)))\n",
        "#8-2. 개발모형 정확도\n",
        "Regressors = [svr,mlp]\n",
        "for Regressor in Regressors :\n",
        "    Regressor.fit(X_train,Y_train)\n",
        "    class_name = Regressor.__class__.__name__\n",
        "    print(\"{0} accuracy : {1:.3f}\".format(class_name, Regressor.score(X_test,Y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ6AvkMVkCpV",
        "outputId": "8bc0fc76-c3ce-49b9-8d68-dbc9ff408a8f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value: \n",
            " [120764.00248238 120761.20327386 120749.93441632 120816.17389589\n",
            " 120706.99883885 120746.38426809 120666.18427871 120641.60855204\n",
            " 120697.80728048 120770.26892128 120776.8271341  120652.3004635\n",
            " 120675.86941783 120763.78919472 120831.31752939 120730.18297748\n",
            " 120753.80787794 120766.09978103 120743.40123382 120750.72892022\n",
            " 120819.76520965 120713.52898205 120776.1450963  120729.25010151\n",
            " 120641.19322179 120755.65664536 120810.79326214 120669.46839575\n",
            " 120659.30322335 120751.27861947 120763.77739319 120813.28626716\n",
            " 120694.26948373 120780.94659187 120750.51936341 120725.65708218\n",
            " 120720.77967767 120788.03852198 120733.83983884 120921.46203611\n",
            " 120748.30243427 120708.95013814 120714.41645266 120727.62835566\n",
            " 120711.27815684 120802.92193115 120684.10476183 120755.46370163\n",
            " 120734.90808532 120781.74046833 120733.10819014 120850.86300633\n",
            " 120622.14629794 120747.10282974 120751.18928271 120743.29978537\n",
            " 120797.25022337 120782.11331318 120737.79739727 120708.8028401\n",
            " 120755.10022574 120754.35603087 120704.9795857  120777.72604135\n",
            " 120710.15416047 120685.05498553 120735.0590102  120677.67968271\n",
            " 120838.64411327 120656.78757279 120770.54351378 120734.70128731\n",
            " 120734.22459331 120717.20856949 120656.96949486 120863.1786622\n",
            " 120808.0175662  120807.75192713 120645.18487117 120780.14925222\n",
            " 120671.89521333 120721.71744265 120714.13220238 120680.24405553\n",
            " 120775.00515878 120764.62645183 120697.40747351 120766.02643559\n",
            " 120715.11644393 120776.03222402 120767.31695562 120776.95971684\n",
            " 120749.35089088 120761.2752851  120728.01134348 120868.89183888\n",
            " 120749.28616767 120674.387347   120776.20152187 120635.09711596\n",
            " 120709.00348982 120749.21476396 120787.04871871 120747.22937208\n",
            " 120770.1608524  120722.67350093 120759.0319752  120707.46943887\n",
            " 120756.46092161 120720.51053839 120731.87142655 120735.40241345\n",
            " 120750.14528764 120847.93558226 120737.55803372 120705.02714016\n",
            " 120742.64371882 120753.69463946 120710.64784455 120725.977347\n",
            " 120786.54884022 120776.94652553 120699.74762828 120724.20976921\n",
            " 120764.94974215 120764.63444582 120675.13803399 120700.02855313\n",
            " 120795.74641202 120741.66820641 120625.31828742 120718.21020416\n",
            " 120826.71316896 120785.72038545 120673.20401848 120752.84696942\n",
            " 120673.34581079 120756.31742709 120757.63561247 120743.88143843\n",
            " 120739.37365553 120744.04384699 120795.64621339 120727.02981965\n",
            " 120688.83406597 120760.00083154 120736.71979468 120758.5368948\n",
            " 120666.47638222 120749.36034971 120686.85370163 120729.53445361\n",
            " 120756.28333245 120776.10226236 120928.79884451 120704.24208084\n",
            " 120605.81864983 120749.88292398 120766.05238047 120740.75072412\n",
            " 120774.30685356 120820.75684969 120814.78179127 120684.77236026\n",
            " 120705.15540144 120776.45505401 120754.14743404 120715.84737087\n",
            " 120792.86270258 120712.30456333 120606.54133012 120716.94007623\n",
            " 120670.06865395 120850.64557225 120671.14625932 120766.13063706\n",
            " 120895.35974673 120709.15210931 120762.44966757 120672.11229931\n",
            " 120871.22811348 120973.71835182 120806.80885745 120835.64870058\n",
            " 120720.47095691 120691.76032509 120759.85395363 120768.37393093\n",
            " 120626.39426869 120769.06744476 120757.3469701  120758.11480295\n",
            " 120891.43524988 120624.65674159 120744.64858287 120755.62265249\n",
            " 120786.55185765 120631.05274311 120773.70685734 120660.49129774\n",
            " 120837.68542577 120636.88415237 120755.30361879 120731.04737155\n",
            " 120712.60987445 120699.92130668 120771.67607943 120774.64565795\n",
            " 120769.9788706  120749.39292257]\n",
            "voting regressor accuracy(R2): -0.475\n",
            "SVR accuracy : -0.052\n",
            "MLPRegressor accuracy : -1.325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#분류예측"
      ],
      "metadata": {
        "id": "vtwBJw75A1D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.변수선택\n",
        "X = df[['총매출액', '구매금액대','1회 평균매출액','평균 구매주기','거래기간']]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3.데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['총매출액','1회 평균매출액','평균 구매주기','거래기간']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['구매금액대'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4.오버샘플링\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
        "\n",
        "#5. 모형 생성\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(random_state = 0, n_estimators=300,max_depth=3)\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value \\n\",Y_pred)\n",
        "print(\"accuracy(test):{:.3f}\".format(model.score(X_test,Y_test)))\n",
        "print(classification_report(Y_test,Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZWut3VEp0kb",
        "outputId": "a14f5786-48e2-483e-a10b-06a0efcba24f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(test):0.947\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.96       177\n",
            "           1       0.99      0.88      0.93       123\n",
            "\n",
            "    accuracy                           0.95       300\n",
            "   macro avg       0.96      0.94      0.94       300\n",
            "weighted avg       0.95      0.95      0.95       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#회귀예측"
      ],
      "metadata": {
        "id": "Vw-gV0r06i2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.변수선택\n",
        "X = df[df.이탈여부 ==0][['방문빈도', '총 할인 금액','고객등급','구매유형','거래기간','할인민감여부','평균 구매주기']]\n",
        "Y = df[df.이탈여부 ==0][\"1회 평균매출액\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['방문빈도', '총 할인 금액','거래기간','평균 구매주기']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['고객등급','구매유형','할인민감여부'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4. 모형 생성\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(random_state = 0, n_estimators=300,max_depth=4)\n",
        "\n",
        "#5. 모형 학습 예측\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value \\n\",Y_pred)\n",
        "print(\"accuracy(R2):{:.3f}\".format(model.score(X_test,Y_test)))\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilxclisD3nt5",
        "outputId": "2e1f73f8-a460-469b-caf4-b3fe7bb3b5e4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [327763.03307519 191964.82436528 180997.99851637 461459.56250676\n",
            " 109740.29112179 268172.8229351   94828.7590478  261457.82737118\n",
            " 151378.28462368 242706.3845063  248839.9325709  246086.28586236\n",
            " 262565.21768027 217822.2607775  337687.02488551 284796.30153296\n",
            " 381298.32300917 390236.02030845 163944.55342271 283793.51464664\n",
            " 334079.76323237 109456.72278846 248839.9325709  341926.47734489\n",
            "  95338.90648162 420565.41977443 331908.68663323 259560.92351783\n",
            " 278771.44474368 182101.41296828 389981.16165529 421754.62575221\n",
            " 276391.96092056 413467.85391942 168341.57282841 164075.25264494\n",
            " 184429.65442031 425700.2220282  157633.41007202 544007.71707449\n",
            " 378147.42829237 209084.99809088 264849.2875702  207138.8211619\n",
            " 265262.17677068 327587.05976706 111143.45185499 184586.5493685\n",
            " 378599.29160175 268465.02302995 151560.93009987 914801.61630232\n",
            "  96202.20807632 246968.56378103 381577.40222994 240414.44603408\n",
            " 320588.8332808  254632.94843079 207971.48818796 252423.44272301\n",
            " 377757.18698549 358911.36890435 205063.7470327  268515.30451978\n",
            " 209084.99809088 148718.47501434 154698.79748314 111260.72362774\n",
            " 475058.98704113 245628.22353722 249080.59190423 136911.44350401\n",
            " 157415.90719137 136212.36318871 287942.50589258 563014.23589229\n",
            " 330637.03607862 444929.80052443 207137.54308442 250654.68713341\n",
            " 284236.79246135 149028.60325849 118235.77878632 262938.83728277\n",
            " 260942.64108588 328047.4507747  335792.35044382 335473.08988054\n",
            " 200380.35683365 410915.47774955 268485.96648508 414674.0950449\n",
            " 316343.58483702 374433.65444066 148943.99719433 590219.7851587\n",
            " 364066.76994863 262565.21768027 418024.13736129 204636.94013655\n",
            " 205762.31678906 283793.51464664 384358.61147594 125440.21633349\n",
            " 239984.17606447 376973.52459236 380456.21839291 145202.08412097\n",
            " 191971.83449742 261903.76882732 164075.25264494 154716.36001518\n",
            " 380071.85475277 737231.92229968 154323.45924201 263018.99431942\n",
            " 270738.7690928  378670.62186169 263018.99431942 343507.27576276\n",
            " 382101.16552553 403631.27599151 209606.69529326 265262.17677068\n",
            " 388627.6095942  239988.61872108 213979.95298029 189723.64647944\n",
            " 322022.62562699 147868.95403232 259330.01653695 135165.91542534\n",
            " 458822.90308533 276268.69247313 112490.76579845 217953.85426106\n",
            " 203967.31312104 191308.4261275  323994.80771528 146157.79199915\n",
            " 248159.95869741 167927.33052683 302827.87193966 147733.52302932\n",
            " 291206.19418243 318659.43261928 378320.21238098 422232.21708663\n",
            " 257234.77619406 163857.74976428 291079.13084909 317181.2093614\n",
            " 321896.49597589 419377.68942237 920788.20544372 110239.97725052\n",
            " 244393.78039522 389617.24600385 217373.39251132 345536.28834571\n",
            " 419449.88497286 334660.94722692 331756.54501058 208017.25946909\n",
            " 264849.2875702  248850.22055377 386824.04267397  95726.28936377\n",
            " 383558.77527073 263018.99431942  97233.05263873 154698.79748314\n",
            " 110197.47021288 796450.83349578 261131.8226412  341258.46486229\n",
            " 992301.27575071 205219.64669018 228662.0663295  203562.41761407\n",
            " 742084.65917944 662084.11261457 329692.4337367  340294.25037105\n",
            " 191726.64872255 146851.08937085 240858.87164365 247974.79071775\n",
            "  93321.78376579 395440.47206608 252879.63142275 380723.96647238\n",
            " 600586.29214665 260096.63964362 163857.74976428 160873.63102682\n",
            " 274746.96759619 259878.48665404 404712.75896661  98439.8647106\n",
            " 461393.3317461  260240.07453255 192461.13671491 123916.74367295\n",
            " 361036.48779689 206216.09342822 244886.23512818 274708.17639334\n",
            " 393763.13038675 283793.51464664]\n",
            "accuracy(R2):0.391\n",
            "RMSE: 203529.14721634527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#그래디언트 부스트(Fradient Boost)"
      ],
      "metadata": {
        "id": "M5oD5ZnNAtmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.변수선택\n",
        "X = df[['총매출액', '구매금액대','1회 평균매출액','평균 구매주기','거래기간']]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3.데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['총매출액','1회 평균매출액','평균 구매주기','거래기간']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['구매금액대'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4.오버샘플링\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
        "\n",
        "#5 모형 생성\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(random_state = 0, n_estimators=100,max_depth=4,learning_rate=0.1)\n",
        "\n",
        "#6 모형 학습 예측\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value \\n\",Y_pred)\n",
        "print(\"accuracy(test):{:.3f}\".format(model.score(X_test,Y_test)))\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VrG-b_gAsJS",
        "outputId": "118e3c21-a15d-488b-afa8-5a445a33edc2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(test):0.933\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94       177\n",
            "           1       0.95      0.89      0.92       123\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.94      0.93      0.93       300\n",
            "weighted avg       0.93      0.93      0.93       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#회귀예측"
      ],
      "metadata": {
        "id": "6pD9PLZVCy-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "g"
      ],
      "metadata": {
        "id": "5vI1a69rCyze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.변수선택\n",
        "X = df[df.이탈여부 ==0][['방문빈도', '총 할인 금액','고객등급','구매유형','거래기간','할인민감여부','평균 구매주기']]\n",
        "Y = df[df.이탈여부 ==0][\"1회 평균매출액\"]\n",
        "\n",
        "#2.데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([('scling', StandardScaler(),['방문빈도', '총 할인 금액','거래기간','평균 구매주기']),\n",
        "                        ('onehot', OneHotEncoder(sparse = False),['고객등급','구매유형','할인민감여부'])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4. 모형생성\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model = GradientBoostingRegressor(random_state = 0, n_estimators=100,max_depth=4,learning_rate=0.1)\n",
        "\n",
        "#5. 모형 학습 예측\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value \\n\",Y_pred)\n",
        "print(\"accuracy(R2):{:.3f}\".format(model.score(X_test,Y_test)))\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ20lFs4Bkzl",
        "outputId": "4d7a1c58-8807-45eb-c192-be38c4c5dc7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [ 324181.19705709  222065.45861667  192973.89797063  528414.33678387\n",
            "  104559.59245935  311617.21585636  182644.05807063  270174.29332734\n",
            "  134923.27319238  284154.68117566  244778.97230368  220073.84403972\n",
            "  237090.87728165  197106.26720129  345944.8049796   250715.97640018\n",
            "  390822.75858976  539794.19503287  144430.54277777  258742.20634313\n",
            "  311659.09367806  123244.69929573  253109.46618558  303830.42085497\n",
            "  141146.09520348  393504.44878848  303530.44715285   64404.78246941\n",
            "  345355.60739172  198430.92266004  355214.57741742  455632.02713022\n",
            "  264232.27360226  573467.12331728   96972.21940292  161509.24413291\n",
            "  177280.92252692  433206.29225938  153158.2245511   545759.78589653\n",
            "  380375.43938088  192258.91984337  286529.32620456  178421.83058451\n",
            "  304338.54534573  316018.54156289   67363.83630274  176358.33572904\n",
            "  326477.61014735  252698.31976339  120502.94873909  758430.09912611\n",
            "   63969.17828605  244112.43098691  445621.39647265  204563.10151275\n",
            "  324290.82851422  245188.3110741   162301.25259607  228336.30423222\n",
            "  370436.28145253  424350.22902303  147713.03424069  263778.70863354\n",
            "  176984.09089883  183005.18165281  143294.77420607  112399.65613649\n",
            "  449140.1853404   255238.93600973  222943.86683167  133361.77555589\n",
            "  142271.94657012  217961.25533856  407716.52246064  626109.2070715\n",
            "  313000.10168054  457624.32936108  287664.88498167  230166.2268504\n",
            "  183148.58147893  142132.75783382  118743.44728564  379937.93280721\n",
            "  247170.14411636  277245.0911761   376277.32603452  306102.30483402\n",
            "  123995.52041519  317301.27490203  267490.11171548  545268.56216554\n",
            "  280060.33766014  474560.478383    119972.23139267  595642.27502784\n",
            "  433334.40389854  203148.51790984  384282.02763669  344807.45490651\n",
            "  129850.87420562  232905.14539487  364590.53929086   87706.60379891\n",
            "  273679.61522575  427841.85454613  395435.40669131  150991.5103769\n",
            "  148288.11451377  300329.05265709  226580.3443578   127763.78323025\n",
            "  348659.60073175  895172.49835952  174641.2131925   214774.22858109\n",
            "  232898.54244303  423364.87867918  201121.20044059  375994.11068295\n",
            "  360800.07128272  368524.2869468   192838.19535981  353520.00080257\n",
            "  351332.80367541  228298.47313544  313588.43722984  134601.41085271\n",
            "  386267.44446403  145358.69328057  314848.98110514  126644.47301636\n",
            "  472789.84247323  293427.89127665  101406.89547333  248139.17204719\n",
            "  113354.48396078  171639.62341587  402615.21620265  132352.99910223\n",
            "  268360.71351247  174719.55179437  329035.91574285   93402.7073899\n",
            "  292199.2030975   333479.03594874  440931.71535139  351199.30398213\n",
            "  262821.42128121  309543.71585369  254333.06847177  421139.71305006\n",
            "  266422.34604871  451669.84989435  938658.63321198  113257.82171486\n",
            "  157028.23281964  415210.55047918  189391.29587136  346739.31496886\n",
            "  448948.8247277   309589.168668    353441.95697755  175292.7261915\n",
            "  328175.34015662  227600.37706175  361499.28930673   77356.95311276\n",
            "  373808.26172982  203040.33970074   82665.28486914  121221.31551185\n",
            "  195260.85958633  819607.59720763  244514.32816219  507129.64993988\n",
            " 1071345.86818867  172485.65372308  426322.4326607   138674.14553266\n",
            "  769461.60204545  637351.25313403  332824.27168869  318351.05577023\n",
            "  197740.68310655  165167.94802477  252849.09130631  262839.44507088\n",
            "   64682.22399773  465329.26369993  237788.07402331  324971.50292855\n",
            "  538396.95701471  282130.61415735  196887.20749433  195306.50508852\n",
            "  296309.0779832   102356.95178193  536702.49013926  160745.54041812\n",
            "  541029.61487277  296259.45832576  174112.76984556  105728.74021137\n",
            "  325319.23428079  155717.66469022  278114.73580897  260524.02627405\n",
            "  406560.01021446  348206.16469192]\n",
            "accuracy(R2):0.401\n",
            "RMSE: 201753.3429283955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xc1PBlp-DVjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}